{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import DataPreprocessing as DP\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.io import fits\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "#from keras.layers import LSTM, Dense, Embedding, Input, Flatten\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM, Flatten, Input, Reshape\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import time\n",
    "Tstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU')]\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for GPU usagegpus = tf.config.experimental.list_physical_devices(device_type='XLA_GPU')\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='XLA_GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_visible_devices(devices=gpus, device_type='XLA_GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5282.0195, 5283.235 , 5284.453 , 5285.669 , 5286.8877, 5288.104 ,\n",
       "        5289.323 , 5290.54  , 5291.7573, 5292.9775, 5294.1953, 5295.416 ,\n",
       "        5296.6343, 5297.8555, 5299.074 , 5300.2935, 5301.5156, 5302.7354,\n",
       "        5303.958 , 5305.178 , 5306.4014, 5307.622 , 5308.8433, 5310.0674,\n",
       "        5311.289 , 5312.5137, 5313.736 , 5314.9614, 5316.184 , 5317.407 ,\n",
       "        5318.6333, 5319.857 , 5321.0835, 5322.3076, 5323.5347, 5324.76  ,\n",
       "        5325.9844, 5327.213 , 5328.438 , 5329.667 , 5330.893 , 5332.122 ,\n",
       "        5333.349 , 5334.5757, 5335.806 , 5337.0337, 5338.264 , 5339.492 ,\n",
       "        5340.7236, 5341.952 , 5343.181 , 5344.413 , 5345.6426, 5346.8755,\n",
       "        5348.1055, 5349.339 , 5350.5693, 5351.8003, 5353.034 , 5354.2656,\n",
       "        5355.5005, 5356.7324, 5357.968 , 5359.2   , 5360.433 , 5361.6694,\n",
       "        5362.903 , 5364.139 , 5365.3735, 5366.611 , 5367.845 , 5369.08  ,\n",
       "        5370.3184, 5371.5537, 5372.7925, 5374.0283, 5375.2676, 5376.504 ,\n",
       "        5377.744 , 5378.981 , 5380.2183, 5381.459 , 5382.6973, 5383.9385,\n",
       "        5385.177 , 5386.419 , 5387.6577, 5388.8975, 5390.14  , 5391.38  ,\n",
       "        5392.623 , 5393.864 , 5395.1074, 5396.3486, 5397.5903, 5398.835 ,\n",
       "        5400.0767, 5401.3223, 5402.5645, 5403.8105, 5405.0537, 5406.297 ,\n",
       "        5407.544 , 5408.7876, 5410.035 , 5411.28  , 5412.5273, 5413.7725,\n",
       "        5415.018 , 5416.2666, 5417.5127, 5418.7617, 5420.0083, 5421.2583,\n",
       "        5422.5054, 5423.753 , 5425.0034, 5426.2515, 5427.503 , 5428.7515,\n",
       "        5430.0034, 5431.2524, 5432.502 , 5433.755 , 5435.005 , 5436.258 ,\n",
       "        5437.509 , 5438.7627, 5440.0137, 5441.265 , 5442.52  , 5443.772 ,\n",
       "        5445.0273, 5446.28  , 5447.5356, 5448.789 , 5450.0425, 5451.2993,\n",
       "        5452.553 , 5453.8105, 5455.0654, 5456.323 , 5457.5786, 5458.834 ,\n",
       "        5460.093 , 5461.3486, 5462.6084, 5463.8647, 5465.125 , 5466.3823,\n",
       "        5467.6396, 5468.9004, 5470.1587, 5471.42  , 5472.6787, 5473.9404,\n",
       "        5475.1997, 5476.4624, 5477.722 , 5478.9824, 5480.246 , 5481.507 ,\n",
       "        5482.7705, 5484.032 , 5485.2964, 5486.5586, 5487.821 , 5489.086 ,\n",
       "        5490.349 , 5491.6147, 5492.8784, 5494.145 , 5495.4087, 5496.673 ,\n",
       "        5497.9404, 5499.2056, 5500.4736, 5501.739 , 5503.0073, 5504.2734,\n",
       "        5505.5396, 5506.8096, 5508.076 , 5509.346 , 5510.614 , 5511.8843,\n",
       "        5513.1523, 5514.421 , 5515.6924, 5516.9614, 5518.2334, 5519.503 ,\n",
       "        5520.776 , 5522.046 , 5523.316 , 5524.59  , 5525.861 , 5527.135 ,\n",
       "        5528.4062, 5529.681 , 5530.9536, 5532.2256, 5533.5015, 5534.7744,\n",
       "        5536.051 ], dtype=float32),\n",
       " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset\n",
    "#norm spectrum\n",
    "kinds = ['boss_cv','boss_da+ms','boss_db','boss_db+ms','boss_dq','boss_dz','fgkm','hotstars','wd','wdsb2','yso','hotstars_m']\n",
    "flux_cv, spectrum_cv = DP.Preprocessing7('/home/njl/ML/optical/'+kinds[0]+'/'+'*.fit')\n",
    "flux_dams, spectrum_dams = DP.Preprocessing7('/home/njl/ML/optical/'+kinds[1]+'/'+'*.fit')\n",
    "flux_db, spectrum_db = DP.Preprocessing7('/home/njl/ML/optical/'+kinds[2]+'/'+'*.fit')\n",
    "\n",
    "#delate bad data\n",
    "n = 0\n",
    "for i in range(len(spectrum_db)):\n",
    "    if len(spectrum_db[i][0]) < 4096:\n",
    "        n = i\n",
    "flux_db.pop(n)\n",
    "spectrum_db.pop(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9712, 1, 4552)\n"
     ]
    }
   ],
   "source": [
    "#input spectrum\n",
    "l_list =  flux_cv+flux_dams+flux_db+flux_cv+flux_dams+flux_db+flux_cv+flux_dams+flux_db+flux_cv+flux_dams+flux_db+flux_cv+flux_dams+flux_db+flux_cv+flux_dams+flux_db+flux_cv+flux_dams+flux_db+flux_cv+flux_dams+flux_db\n",
    "sl = []\n",
    "for i in range(len(l_list)):\n",
    "    sl.append(len(l_list[i]))\n",
    "sl_min = np.min(sl)\n",
    "X_train = []\n",
    "for i in range(len(l_list)):\n",
    "    X_train.append(l_list[i][0:sl_min])\n",
    "X_train = np.array(X_train)\n",
    "size = 4552\n",
    "#X_train = l_list\n",
    "X_train = np.stack(X_train)\n",
    "#X_train = X_train.reshape(len(X_train),size,1) #change the shape to NHWC for CAE input\n",
    "X_train = X_train.reshape(len(X_train),1,size)#(1214,1,4552)\n",
    "print(X_train.shape) #print information of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4552)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1, 4552])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=X_train.shape[1:])#(1214,4552)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################model\n",
    "#LSTM net\n",
    "# x = LSTM(4096,activation='tanh',return_sequences=True)(inputs)#(256)\n",
    "# #x = Dropout(0.001)(x)#(256)\n",
    "x = LSTM(2048,activation='tanh',return_sequences=True)(inputs)#(128)\n",
    "#x = Dropout(0.001)(x)#(128)\n",
    "x = LSTM(1024,activation='tanh')(x)#(128)\n",
    "#x = Dropout(0.01)(x)#(128)\n",
    "# x = LSTM(512,activation='tanh')(x)#(128)\n",
    "#x = Dropout(0.01)(x)#(128)\n",
    "# #hidden net\n",
    "x = Dense(64, activation='relu')(x)#(2048)\n",
    "x = Dense(16, activation='relu')(x)#(1024)\n",
    "x = Dense(4, activation='relu')(x)#(1024)\n",
    "encoded = Dense(1, activation='relu', name='embedding')(x)#(2)\n",
    "x = Dense(4, activation='relu')(encoded)#(1024)\n",
    "x = Dense(16, activation='relu')(x)#(2048)\n",
    "x = Dense(64, activation='relu')(x)#(2048)\n",
    "x = Dense(1024, activation='relu')(x)#(2048)\n",
    "decoded = Dense(4552, activation='softmax')(x)#(4552)\n",
    "x = Reshape((1,4552))(decoded)#(1,4552)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1, 4552)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 2048)           54075392  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1024)              12587008  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                800       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2048)              264192    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4552)              9327048   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 1, 4552)           0         \n",
      "=================================================================\n",
      "Total params: 76,407,200\n",
      "Trainable params: 76,407,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Model(inputs, x)\n",
    "#autoencoder = multi_gpu_model(input_spe, decoded)\n",
    "optimizer_adam = optimizers.Adam(lr=0.001)#learning rate\n",
    "autoencoder.compile(optimizer=optimizer_adam, loss='categorical_crossentropy', metrics=['accuracy'])####loss function\n",
    "#autoencoder.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepochs = 300 #number of epochs for CAE training\n",
    "tosavemodel = True #if save the trained CAE model\n",
    "plot_reconstruction = True #if plot the reconstruction comparison\n",
    " #setup if \"tosavemodel=True\" or \"plot_reconstuction=True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ## DATE PREPARATION RUNTIME: 705.0855419635773\n",
      "Epoch 1/300\n",
      "19/19 - 1s - loss: 35288.1641 - accuracy: 0.0063\n",
      "Epoch 2/300\n",
      "19/19 - 1s - loss: 33067.1953 - accuracy: 0.0091\n",
      "Epoch 3/300\n",
      "19/19 - 1s - loss: 32794.6719 - accuracy: 0.0146\n",
      "Epoch 4/300\n",
      "19/19 - 1s - loss: 32581.3203 - accuracy: 0.0233\n",
      "Epoch 5/300\n",
      "19/19 - 1s - loss: 32408.6484 - accuracy: 0.0273\n",
      "Epoch 6/300\n",
      "19/19 - 1s - loss: 32256.3398 - accuracy: 0.0340\n",
      "Epoch 7/300\n",
      "19/19 - 1s - loss: 32064.1152 - accuracy: 0.0408\n",
      "Epoch 8/300\n",
      "19/19 - 1s - loss: 31936.1875 - accuracy: 0.0459\n",
      "Epoch 9/300\n",
      "19/19 - 1s - loss: 31858.8242 - accuracy: 0.0510\n",
      "Epoch 10/300\n",
      "19/19 - 1s - loss: 31842.2012 - accuracy: 0.0550\n",
      "Epoch 11/300\n",
      "19/19 - 1s - loss: 31677.7598 - accuracy: 0.0626\n",
      "Epoch 12/300\n",
      "19/19 - 1s - loss: 31591.2090 - accuracy: 0.0659\n",
      "Epoch 13/300\n",
      "19/19 - 1s - loss: 31575.1641 - accuracy: 0.0703\n",
      "Epoch 14/300\n",
      "19/19 - 1s - loss: 31640.3633 - accuracy: 0.0707\n",
      "Epoch 15/300\n",
      "19/19 - 1s - loss: 31562.3594 - accuracy: 0.0696\n",
      "Epoch 16/300\n",
      "19/19 - 1s - loss: 31444.6035 - accuracy: 0.0830\n",
      "Epoch 17/300\n",
      "19/19 - 1s - loss: 31369.7793 - accuracy: 0.0856\n",
      "Epoch 18/300\n",
      "19/19 - 1s - loss: 31271.5723 - accuracy: 0.0951\n",
      "Epoch 19/300\n",
      "19/19 - 1s - loss: 31213.8281 - accuracy: 0.0998\n",
      "Epoch 20/300\n",
      "19/19 - 1s - loss: 31149.0117 - accuracy: 0.1021\n",
      "Epoch 21/300\n",
      "19/19 - 1s - loss: 31108.2246 - accuracy: 0.1026\n",
      "Epoch 22/300\n",
      "19/19 - 1s - loss: 31074.8340 - accuracy: 0.1041\n",
      "Epoch 23/300\n",
      "19/19 - 1s - loss: 31048.5176 - accuracy: 0.1077\n",
      "Epoch 24/300\n",
      "19/19 - 1s - loss: 31046.0527 - accuracy: 0.1081\n",
      "Epoch 25/300\n",
      "19/19 - 1s - loss: 31062.3359 - accuracy: 0.1072\n",
      "Epoch 26/300\n",
      "19/19 - 1s - loss: 30994.1680 - accuracy: 0.1126\n",
      "Epoch 27/300\n",
      "19/19 - 1s - loss: 30950.4473 - accuracy: 0.1170\n",
      "Epoch 28/300\n",
      "19/19 - 1s - loss: 30909.6934 - accuracy: 0.1216\n",
      "Epoch 29/300\n",
      "19/19 - 1s - loss: 30880.6914 - accuracy: 0.1237\n",
      "Epoch 30/300\n",
      "19/19 - 1s - loss: 30845.2227 - accuracy: 0.1206\n",
      "Epoch 31/300\n",
      "19/19 - 1s - loss: 30805.8125 - accuracy: 0.1224\n",
      "Epoch 32/300\n",
      "19/19 - 1s - loss: 30785.7695 - accuracy: 0.1269\n",
      "Epoch 33/300\n",
      "19/19 - 1s - loss: 30787.4336 - accuracy: 0.1223\n",
      "Epoch 34/300\n",
      "19/19 - 1s - loss: 30775.6875 - accuracy: 0.1191\n",
      "Epoch 35/300\n",
      "19/19 - 1s - loss: 30730.7188 - accuracy: 0.1260\n",
      "Epoch 36/300\n",
      "19/19 - 1s - loss: 30741.3867 - accuracy: 0.1273\n",
      "Epoch 37/300\n",
      "19/19 - 1s - loss: 30728.8047 - accuracy: 0.1243\n",
      "Epoch 38/300\n",
      "19/19 - 1s - loss: 30712.9824 - accuracy: 0.1288\n",
      "Epoch 39/300\n",
      "19/19 - 1s - loss: 30726.1641 - accuracy: 0.1272\n",
      "Epoch 40/300\n",
      "19/19 - 1s - loss: 30674.0957 - accuracy: 0.1277\n",
      "Epoch 41/300\n",
      "19/19 - 1s - loss: 30634.9395 - accuracy: 0.1340\n",
      "Epoch 42/300\n",
      "19/19 - 1s - loss: 30634.7852 - accuracy: 0.1271\n",
      "Epoch 43/300\n",
      "19/19 - 1s - loss: 30621.1465 - accuracy: 0.1325\n",
      "Epoch 44/300\n",
      "19/19 - 1s - loss: 30622.6562 - accuracy: 0.1307\n",
      "Epoch 45/300\n",
      "19/19 - 1s - loss: 30597.8711 - accuracy: 0.1322\n",
      "Epoch 46/300\n",
      "19/19 - 1s - loss: 30563.7070 - accuracy: 0.1393\n",
      "Epoch 47/300\n",
      "19/19 - 1s - loss: 30766.4980 - accuracy: 0.1185\n",
      "Epoch 48/300\n",
      "19/19 - 1s - loss: 30992.3164 - accuracy: 0.1107\n",
      "Epoch 49/300\n",
      "19/19 - 1s - loss: 30782.7539 - accuracy: 0.1109\n",
      "Epoch 50/300\n",
      "19/19 - 1s - loss: 30674.7676 - accuracy: 0.1240\n",
      "Epoch 51/300\n",
      "19/19 - 1s - loss: 30590.1738 - accuracy: 0.1375\n",
      "Epoch 52/300\n",
      "19/19 - 1s - loss: 30548.7617 - accuracy: 0.1419\n",
      "Epoch 53/300\n",
      "19/19 - 1s - loss: 30521.8887 - accuracy: 0.1448\n",
      "Epoch 54/300\n",
      "19/19 - 1s - loss: 30505.3320 - accuracy: 0.1531\n",
      "Epoch 55/300\n",
      "19/19 - 1s - loss: 30489.8047 - accuracy: 0.1541\n",
      "Epoch 56/300\n",
      "19/19 - 1s - loss: 30498.7812 - accuracy: 0.1473\n",
      "Epoch 57/300\n",
      "19/19 - 1s - loss: 30490.9453 - accuracy: 0.1431\n",
      "Epoch 58/300\n",
      "19/19 - 1s - loss: 30480.3008 - accuracy: 0.1482\n",
      "Epoch 59/300\n",
      "19/19 - 1s - loss: 30476.3945 - accuracy: 0.1501\n",
      "Epoch 60/300\n",
      "19/19 - 1s - loss: 30481.5254 - accuracy: 0.1455\n",
      "Epoch 61/300\n",
      "19/19 - 1s - loss: 30480.7578 - accuracy: 0.1457\n",
      "Epoch 62/300\n",
      "19/19 - 1s - loss: 30464.5371 - accuracy: 0.1449\n",
      "Epoch 63/300\n",
      "19/19 - 1s - loss: 30456.7324 - accuracy: 0.1482\n",
      "Epoch 64/300\n",
      "19/19 - 1s - loss: 30462.5508 - accuracy: 0.1485\n",
      "Epoch 65/300\n",
      "19/19 - 1s - loss: 30456.3203 - accuracy: 0.1487\n",
      "Epoch 66/300\n",
      "19/19 - 1s - loss: 30435.7129 - accuracy: 0.1542\n",
      "Epoch 67/300\n",
      "19/19 - 1s - loss: 30424.3633 - accuracy: 0.1526\n",
      "Epoch 68/300\n",
      "19/19 - 1s - loss: 30421.6602 - accuracy: 0.1482\n",
      "Epoch 69/300\n",
      "19/19 - 1s - loss: 30419.8750 - accuracy: 0.1494\n",
      "Epoch 70/300\n",
      "19/19 - 1s - loss: 30406.7188 - accuracy: 0.1525\n",
      "Epoch 71/300\n",
      "19/19 - 1s - loss: 30401.4043 - accuracy: 0.1512\n",
      "Epoch 72/300\n",
      "19/19 - 1s - loss: 30400.6289 - accuracy: 0.1533\n",
      "Epoch 73/300\n",
      "19/19 - 1s - loss: 30395.5215 - accuracy: 0.1561\n",
      "Epoch 74/300\n",
      "19/19 - 1s - loss: 30387.5430 - accuracy: 0.1527\n",
      "Epoch 75/300\n",
      "19/19 - 1s - loss: 30383.6445 - accuracy: 0.1560\n",
      "Epoch 76/300\n",
      "19/19 - 1s - loss: 30383.8750 - accuracy: 0.1541\n",
      "Epoch 77/300\n",
      "19/19 - 1s - loss: 30385.8574 - accuracy: 0.1525\n",
      "Epoch 78/300\n",
      "19/19 - 1s - loss: 30389.4668 - accuracy: 0.1553\n",
      "Epoch 79/300\n",
      "19/19 - 1s - loss: 30384.9922 - accuracy: 0.1516\n",
      "Epoch 80/300\n",
      "19/19 - 1s - loss: 30381.8086 - accuracy: 0.1533\n",
      "Epoch 81/300\n",
      "19/19 - 1s - loss: 30384.7090 - accuracy: 0.1491\n",
      "Epoch 82/300\n",
      "19/19 - 1s - loss: 30367.1074 - accuracy: 0.1567\n",
      "Epoch 83/300\n",
      "19/19 - 1s - loss: 30371.1562 - accuracy: 0.1505\n",
      "Epoch 84/300\n",
      "19/19 - 1s - loss: 30380.0859 - accuracy: 0.1480\n",
      "Epoch 85/300\n",
      "19/19 - 1s - loss: 30367.5840 - accuracy: 0.1509\n",
      "Epoch 86/300\n",
      "19/19 - 1s - loss: 30367.3242 - accuracy: 0.1468\n",
      "Epoch 87/300\n",
      "19/19 - 1s - loss: 30362.4277 - accuracy: 0.1515\n",
      "Epoch 88/300\n",
      "19/19 - 1s - loss: 30355.9434 - accuracy: 0.1548\n",
      "Epoch 89/300\n",
      "19/19 - 1s - loss: 30349.2793 - accuracy: 0.1499\n",
      "Epoch 90/300\n",
      "19/19 - 1s - loss: 30348.0762 - accuracy: 0.1544\n",
      "Epoch 91/300\n",
      "19/19 - 1s - loss: 30353.1504 - accuracy: 0.1596\n",
      "Epoch 92/300\n",
      "19/19 - 1s - loss: 30356.9590 - accuracy: 0.1546\n",
      "Epoch 93/300\n",
      "19/19 - 1s - loss: 30350.8203 - accuracy: 0.1458\n",
      "Epoch 94/300\n",
      "19/19 - 1s - loss: 30338.4082 - accuracy: 0.1532\n",
      "Epoch 95/300\n",
      "19/19 - 1s - loss: 30344.2891 - accuracy: 0.1514\n",
      "Epoch 96/300\n",
      "19/19 - 1s - loss: 30340.1621 - accuracy: 0.1549\n",
      "Epoch 97/300\n",
      "19/19 - 1s - loss: 30328.3730 - accuracy: 0.1592\n",
      "Epoch 98/300\n",
      "19/19 - 1s - loss: 30318.9355 - accuracy: 0.1608\n",
      "Epoch 99/300\n",
      "19/19 - 1s - loss: 30319.4258 - accuracy: 0.1597\n",
      "Epoch 100/300\n",
      "19/19 - 1s - loss: 30323.2285 - accuracy: 0.1562\n",
      "Epoch 101/300\n",
      "19/19 - 1s - loss: 30319.3105 - accuracy: 0.1563\n",
      "Epoch 102/300\n",
      "19/19 - 1s - loss: 30327.3418 - accuracy: 0.1601\n",
      "Epoch 103/300\n",
      "19/19 - 1s - loss: 30322.2070 - accuracy: 0.1599\n",
      "Epoch 104/300\n",
      "19/19 - 1s - loss: 30328.5957 - accuracy: 0.1555\n",
      "Epoch 105/300\n",
      "19/19 - 1s - loss: 30337.8906 - accuracy: 0.1516\n",
      "Epoch 106/300\n",
      "19/19 - 1s - loss: 30343.2148 - accuracy: 0.1483\n",
      "Epoch 107/300\n",
      "19/19 - 1s - loss: 30335.6895 - accuracy: 0.1499\n",
      "Epoch 108/300\n",
      "19/19 - 1s - loss: 30314.2832 - accuracy: 0.1561\n",
      "Epoch 109/300\n",
      "19/19 - 1s - loss: 30301.9043 - accuracy: 0.1626\n",
      "Epoch 110/300\n",
      "19/19 - 1s - loss: 30300.6895 - accuracy: 0.1621\n",
      "Epoch 111/300\n",
      "19/19 - 1s - loss: 30298.3125 - accuracy: 0.1616\n",
      "Epoch 112/300\n",
      "19/19 - 1s - loss: 30298.3066 - accuracy: 0.1650\n",
      "Epoch 113/300\n",
      "19/19 - 1s - loss: 30297.3574 - accuracy: 0.1624\n",
      "Epoch 114/300\n",
      "19/19 - 1s - loss: 30291.2559 - accuracy: 0.1644\n",
      "Epoch 115/300\n",
      "19/19 - 1s - loss: 30288.4258 - accuracy: 0.1658\n",
      "Epoch 116/300\n",
      "19/19 - 1s - loss: 30300.4453 - accuracy: 0.1623\n",
      "Epoch 117/300\n",
      "19/19 - 1s - loss: 30305.0156 - accuracy: 0.1589\n",
      "Epoch 118/300\n",
      "19/19 - 1s - loss: 30311.0371 - accuracy: 0.1601\n",
      "Epoch 119/300\n",
      "19/19 - 1s - loss: 30306.0430 - accuracy: 0.1549\n",
      "Epoch 120/300\n"
     ]
    }
   ],
   "source": [
    "Tprocess0 = time.time()\n",
    "print('\\n', '## DATE PREPARATION RUNTIME:', Tprocess0-Tstart) #Timer\n",
    "\n",
    "## MAIN ##\n",
    "#training\n",
    "H = autoencoder.fit(X_train, X_train,\n",
    "                    batch_size = 512,\n",
    "                    epochs=Nepochs,\n",
    "                    verbose=2,\n",
    "                    shuffle=True)\n",
    "\n",
    "Tprocess1 = time.time()\n",
    "print('\\n', '## AE TRAINING RUNTIME:', Tprocess1-Tprocess0) #Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n', '## AE TRAINING RUNTIME:', Tprocess1-Tprocess0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss function and accuracy function\n",
    "acc = H.history['accuracy']\n",
    "loss = H.history['loss']\n",
    "plt.figure(figsize=[10,5])\n",
    "font2 = {'family' : 'Times New Roman',\n",
    "    'weight' : 'normal',\n",
    "    'size' : 16,\n",
    "    }\n",
    "\n",
    "epochs = range(1,Nepochs+1)\n",
    " \n",
    "#plt.title('Accuracy')\n",
    "plt.plot(epochs, acc,'red')\n",
    "plt.xlabel('epoch',font2)\n",
    "plt.ylabel('accuracy',font2)\n",
    "plt.tick_params(labelsize=12)\n",
    "\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss function and accuracy function\n",
    " \n",
    "#plt.title('Loss')\n",
    "plt.figure(figsize=[10,5])\n",
    "plt.plot(epochs, loss, 'blue')\n",
    "plt.xlabel('epoch',font2)\n",
    "plt.ylabel('loss',font2)\n",
    "plt.tick_params(labelsize=12)\n",
    "\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = 'LAE_reconstruction'\n",
    "tosavemodel = True\n",
    "if tosavemodel:\n",
    "    #restore the model\n",
    "    autoencoder.save(savename + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
